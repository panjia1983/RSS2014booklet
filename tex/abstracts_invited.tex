%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Invited Talks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%some helper commands for common formatting:
\newcommand{\invitedTalk}[7]{
 \begin{minipage}[t]{0.48\textwidth}
 \vspace{1em}
 \begingroup
  \sf
  {\Large #4\\}
  {\LARGE #1\\}
  \begin{spacing}{1.0}

     {\LARGE \raggedright #2}
     
  \end{spacing}
  %\vspace{5mm}
  \endgroup
  \normalsize
  #6
  
 \end{minipage}
 ~\hfill~
 \begin{minipage}[t]{0.43\textwidth}
  \begin{flushright}
   \vspace{1em}
     \ifthenelse { \equal{#5}{} } {
	%\vspace{2cm}
     } {
              \includegraphics[height=5cm]{local_img/speakers/#5}\\
     }
     \textbf{#1}\\
     #3 \\[5mm]
  \end{flushright}
  {\small \textbf{Biography}
  
  #7
  
  }
\end{minipage}
\vspace{1cm}
%\clearpage

}


\vspace{-2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\invitedTalk{Chris Urmson}{Realizing Self-Driving Cars}{Google [x]}{Monday, July 14, 09:00}{urmson.jpg}
{
Self-driving vehicles are coming. They will save lives, save time and offer mobility to those who otherwise don't have it. Eventually they will reshape the way we live in, and move through, our communities and cities. Or so the story goes. Is this a 50 year pipe dream, or a near term reality? A dedicated team at Google has spent the last five years moving self-driving vehicles closer to a reality. New algorithms, increased processing power, innovative sensors and massive amounts of data enable our vehicles to see further, understand more and handle a wide variety of challenging driving scenarios. Our vehicles have driven over a half million miles on highways, suburban and urban streets. Through this journey, we've learned a lot; not just about how to drive, but about interacting with drivers, users and others on the road, and about what it takes to bring in incredibly complex system to fruition. I'll share some fun stories and lessons along with our vision for how these vehicles will become a reality.
}{
Chris Urmson leads Google's self-driving car program where the team's vehicles have driven over a half million miles. Prior to joining Google, Chris was on the faculty of the Robotics Institute at Carnegie Mellon University where his research focused on motion planning and perception for robotic vehicles. During his time at Carnegie Mellon, he worked with house size trucks, drove robots around in deserts and served as the Director of Technology for the team that won the 2007 DARPA Urban Challenge. He earned his PhD in 2005 from Carnegie Mellon and his B.Sc. in Computer Engineering from the University of Manitoba in 1998.
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\invitedTalk{Genevieve Bell}{The Pre-history of Robots}{Intel}{Monday, July 14, 14:00}{bell.jpg}
{
Genevieve Bell is a well known anthropologist, working at the intersection of culture and technology. Australian by birth, Bell runs one of Intel's premier research and development labs in the United States. In this talk, she unpacks the relationship between technology and anxiety, tracing out the origins of our socio-technical fears. Drawing on cultural, literary and historical accounts, Bell makes the case that we have an opportunity to re-imagine the ways in which we encounter and make sense of new digital technologies.
}{
Dr. Genevieve Bell is an anthropologist and researcher with 15 years of experience driving innovation in the high tech industry. As the Director of Interaction and Experience Research in Intel Labs, Bell leads a team of social scientists, interaction designers, human factors engineers and computer scientists. This organization researches new computing experiences that are centered around people's needs and desires. This foundationally shapes and then helps to create new Intel technologies and products. In this team and her prior roles, Bell has fundamentally altered the way Intel envisions and plans its products so that they are centered on people's needs rather than simply silicon capabilities. In addition to leading this increasingly important area at Intel, Bell is an accomplished industry commentator on the intersection of culture and technology and has been extensively featured in publications that include Wired, Forbes, The Atlantic, Fast Company, and the Wall Street Journal. In August 2013, Fast Company declared her to be one of the top 25 smartest women on Twitter, where she goes by the handle, @feraldata. She is a regular public speaker and panelist at technology conferences worldwide, sharing myriad insights gained from her extensive international field work and research. In 2010, Bell was named one of Fast Company's inaugural '100 Most Creative People in Business.' Bell is a passionate advocate for the advancement of women in technology and in 2012 was inducted into the Women In Technology International (WITI) hall of fame, as well being honored by the Anita Borg Institute as the 2013 Woman of Vision for Leadership. Her first book, 'Divining the Digital Future: Mess and Mythology in Ubiquitous Computing,' was co-written with Prof. Paul Dourish of the University of California at Irvine and released in April 2011. Bell is also the recipient of several patents for consumer electronics innovations. A native of Australia, Bell moved to the United States for her undergraduate studies and graduated from Bryn Mawr in 1990 with a bachelor's degree in anthropology. She then earned a master's degree and a doctorate in cultural anthropology from Stanford University where she also taught as an acting lecturer in the Department of Anthropology from 1996-1998. With a father who was an engineer and a mother who was an anthropologist, perhaps Bell was fated to ultimately work for a technology company, joining Intel in 1998. Bell is currently an Intel Fellow.
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\invitedTalk{Brad Nelson}{Swimming Microrobots}{ETH Zurich}{Tuesday, July 15, 09:00}{nelson.jpg}
{
Nature has inspired numerous microrobotic locomotion designs that are suitable for propulsion generation at low Reynolds numbers. This talk first reviews various swimming methods with a particular focus on helical propulsion inspired by E. coli bacteria. To actuate swimming microrobots, various magnetic actuation methods have been proposed, such as rotating fields, oscillating fields, and field gradients. These methods can be categorized into force-driven or torque-driven actuation. It can be shown that torque-driven approaches scale better to the micro- and nano-scale than force-driven approaches. The implementation of swarm or multi-agent control will also be discussed. The use of multiple microrobots may be beneficial for in vivo as well as in vitro applications, and the frequency-dependent behavior of helical microrobots allows individual agents to be decoupled from within small groups. Finally, an elegant commercial application of microrobots originally inspired by helical swimmers will be presented.
}{
Brad Nelson is the Professor of Robotics and Intelligent Systems at ETH Zurich. His primary research focus is on microrobotics and nanorobotics emphasizing applications in biology and medicine. He received mechanical engineering degrees from the University of Illinois (B.S. 1984) and the University of Minnesota (M.S. 1987) and a Ph.D. in Robotics (School of Computer Science) from Carnegie Mellon University (1995). He has worked as an engineer at Honeywell and Motorola and served as a United States Peace Corps Volunteer in Botswana, Africa. He was an Assistant Professor at the University of Illinois at Chicago (1995-1998) and an Associate Professor at the University of Minnesota (1998-2002). He became a Full Professor at ETH Zurich in 2002. Prof. Nelson has received a number of awards including more than a dozen Best Paper Awards at major robotics conferences and journals. He was named to the 2005 "Scientific American 50," Scientific American magazine's annual list recognizing fifty outstanding acts of leadership in science and technology from the past year for his efforts in nanotube manufacturing. His laboratory won the 2007 and 2009 RoboCup Nanogram Competition, both times the event has been held. He is a European Research Council Advanced Grantee (2011) and his lab appears in the 2012 Guinness Book of World Records for the "Most Advanced Mini Robot for Medical Use." In 2013 he was listed as an ISI Highly Cited Researcher. He serves on the editorial boards of several journals, has chaired several international workshops and conferences, has served as the head of the ETH Department of Mechanical and Process Engineering, the Chairman of the ETH Electron Microscopy Center (EMEZ), and is a member of the Research Council of the Swiss National Science Foundation.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\invitedTalk{Andrew Ng}{Deep Learning: Machine Learning via Large-scale Brain Simulations}{Stanford University}{Wednesday, July 16, 09:00}{ng.jpg}
{
Machine learning is a very successful technology, but applying it to a new problem usually means spending a long time hand designing the input features to feed to the learning algorithm. This is true for applications in vision, audio, and text/NLP. To address this, researchers in machine learning have recently developed "deep learning" algorithms, which can automatically learn feature representations from unlabeled data, thus bypassing most of this time-consuming engineering. These algorithms are based on building massive artificial neural networks that were loosely inspired by cortical (brain) computations. In this talk, I describe the key ideas behind deep learning, and also discuss the computational challenges of getting these algorithms to work. I'll also present a few case studies, and report on the results from a project that I led at Google to build massive deep learning algorithms, resulting in a highly distributed neural network trained on 16,000 CPU cores, and that learned by itself to discover high level concepts such as common objects in video.
}{
Andrew Ng's research is in the areas of machine learning and artificial intelligence. Through building very large-scale cortical (brain) simulations, he is developing algorithms that can learn to sense and perceive without needing to be explicitly programed. Using these techniques, he has developed sophisticated computer vision algorithms, as well as a variety of highly capable robots, such as by far the most advanced autonomous helicopter controller, that is able to fly spectacular aerobatic maneuvers. His group at Stanford University (together with Willow Garage) also developed ROS, which is today by far the most widely used open-source robotics software platform. In 2011, he taught an online Machine Learning class to over 100,000 students, leading to the founding of Coursera, which is today the world's largest MOOC platform. Ng has also been named to the 2013 "Time 100" list of the most influential people in the world.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\invitedTalk{Nancy Amato}{Using Motion Planning to Study Protein Motions}{Texas A\&M University}{Wednesday, July 16, 14:00}{amato.jpg}
{
Protein motions, ranging from molecular flexibility to large-scale conformational change, play an essential role in many biochemical processes. For example, some devastating diseases such as Alzheimer's and bovine spongiform encephalopathy (Mad Cow) are associated with the misfolding of proteins. Despite the explosion of structural and functional data, our understanding of protein movement is still very limited because it is difficult to measure experimentally and computationally expensive to simulate. In this talk, we describe how techniques developed for motion planning in robotics have been adapted and applied to model and analyze protein motions and to reason about the structure, flexibility and interactions of proteins and other biomolecules. These techniques adapt sampling-based methods developed for robotic configuration spaces to construct approximate maps of a protein's potential energy landscape which can be used, e.g., to generate transitional motions of a protein to the native state from unstructured conformations or between specified conformations. For example, we show how our map-based tools for modeling and analyzing folding landscapes can capture subtle folding differences between protein G and its mutants, NuG1 and NuG2.
}{
Nancy M. Amato is Unocal Professor and Interim Department Head of the Department of Computer Science and Engineering at Texas A\&M University where she co-directs the Parasol Lab. She received undergraduate degrees in Mathematical Sciences and Economics from Stanford University, and M.S. and Ph.D. degrees in Computer Science from UC Berkeley and the University of Illinois at Urbana-Champaign. Her main areas of research focus are motion planning and robotics, computational biology and geometry, and parallel and distributed computing. She was Editor-in-Chief of the IEEE/RSJ IROS Conference Paper Review Board from 2011-2013, has served on the editorial boards of the IEEE Transactions of Robotics and Automation, IEEE Transactions on Parallel and Distributed Computing, and is an elected member of the Administrative Committee of the IEEE Robotics and Automation Society. She was co-Chair of the NCWIT Academic Alliance (2009-2011), is a member of the Computing Research Association's Committee on the Status of Women in Computing Research (CRA-W) and of the Coalition to Diversity Computing (CDC). She was an AT\&T Bell Laboratories PhD Scholar, received an NSF CAREER Award, is a Distinguished Speaker for the ACM Distinguished Speakers Program, and was a Distinguished Lecturer for the IEEE Robotics and Automation Society. She received the 2013 IEEE Hewlett-Packard/Harriet B. Rigas Award, and a University-level teaching award and the Betty M. Unterberger Award for Outstanding Service to Honors Education at Texas A\&M. She is a AAAS Fellow and an IEEE Fellow.
}


\chapter{Early Career Spotlight}

\invitedTalk{Siddhartha Srinivasa}{The Mathematics of Human Robot Interaction}{Carnegie Mellon University}{Monday, June 24, 15:30}{srinivasa.jpg}
{
For years, the focus of robot motion planning has been to produce functional motion: industrial robots move to weld parts, vacuuming robots move to suck dust, and personal robots move to clean up a dirty table. We have been exploring the thesis that although functional motion is ideal when robots perform tasks in isolation, it is insufficient for collaboration, where a human and a robot are manipulating in a tightly-coupled shared workspace. Our goal is to make this collaboration fluent and seamless. To this end, we have been developing algorithms where the notion of an observer watching the motion is woven into the fabric of the motion planner. This perspective has allowed us to formalize qualitative notions such as predictability and legibility in psychology in terms of Bayesian inference and inverse optimal control, and to develop generative models for such motion using functional gradient optimization. I will also describe some of our user studies on applying these algorithms to human-robot handovers, 
assistive teleoperation, and shared workspace collaboration, and ongoing work on deception, ambiguity, and emotive motion.
}{
Siddhartha Srinivasa is an Associate Professor at the Robotics Institute at Carnegie Mellon University. His research focuses on manipulation, with the goal of enabling robots to robustly and gracefully interact with the world to perform complex manipulation tasks in uncertain, unstructured, and cluttered environments. His current research focuses on physics-based nonprehensile manipulation for reconfiguring clutter, functional gradient methods for motion planning, and formalizing HRI principles using machine learning, motion planning and optimization algorithms. Sidd is a recipient of the HRI Best Paper Award (2010), ONR Young Investigator Award (2012), Okawa Research Award (2012), and a best-paper finalist at RSS (2012, 2013), IEEE ICRA (2009, 2010, 2012), IEEE IROS (2010), and RO-MAN (2012). Sidd also captains the CMU squash team, and dreams of running ultra-marathons.
}


\invitedTalk{Hadas Kress-Gazit}{High-Level Verifiable Robotics}{Cornell University}{Monday, June 24, 16:30}{kress-gazit.jpg}
{
Why don’t we have robots fetching us coffee and finding our keys for us? While robots have become more capable and powerful, they are not yet integrated into everyday life. Part of the reason for this is that robots are difficult to program and even more difficult to verify. Therefore, to achieve the dream of a robot in every home, two key challenges must be addressed; people should be able to easily interact with robots, and robots must always do as they are told.

In this talk I will discuss the work done in my group to address these challenges. Specifically, I will describe the use of language and temporal logic to capture high-level task specifications and the development of formal methods that take the task specifications and produce correct robot behavior, if such behavior exists.
}{
Hadas Kress-Gazit is an Assistant Professor at the Sibley School of Mechanical and Aerospace Engineering at Cornell University. She received her Ph.D. in Electrical and Systems Engineering from the University of Pennsylvania in 2008 and has been at Cornell since 2009. Her research focuses on formal methods for robotics and automation and more specifically on creating verifiable robot controllers for complex high-level tasks using logic, verification, synthesis, hybrid systems theory and computational linguistics. She received an NSF CAREER award in 2010 and a DARPA Young Faculty Award in 2012.
}
